{
  "agent_id": 20,
  "name": "AI Safety Detection",
  "file": "api/src/services/ai-safety-detection.service.ts",
  "status": "success",
  "content": "/**\n * AI Safety Detection Service\n * Advanced computer vision for driver safety monitoring\n * Detects: distracted driving, drowsiness, phone use, seatbelt violations, smoking\n *\n * NOTE: TensorFlow.js dependencies removed for build compatibility\n * Uses external AI APIs (OpenAI/Azure) instead\n */\n\n// import * as tf from '@tensorflow/tfjs-node'; // Removed for build compatibility\nimport axios from 'axios';\nimport { Pool } from 'pg';\n\nimport logger from '../config/logger';\n\nconst OPENAI_API_KEY = process.env.OPENAI_API_KEY;\nconst AZURE_COMPUTER_VISION_KEY = process.env.AZURE_COMPUTER_VISION_KEY;\nconst AZURE_COMPUTER_VISION_ENDPOINT = process.env.AZURE_COMPUTER_VISION_ENDPOINT;\n\ninterface DetectionResult {\n  detections: Detection[];\n  riskScore: number;\n  confidence: number;\n  processingTime: number;\n}\n\ninterface Detection {\n  type: string;\n  category: 'distraction' | 'drowsiness' | 'violation' | 'unsafe_driving';\n  severity: 'minor' | 'moderate' | 'severe' | 'critical';\n  confidence: number;\n  boundingBox?: BoundingBox;\n  metadata?: any;\n}\n\ninterface BoundingBox {\n  x: number;\n  y: number;\n  width: number;\n  height: number;\n}\n\nclass AISafetyDetectionService {\n  private db: Pool;\n  // private model: tf.GraphModel | null = null; // TensorFlow removed\n  private model: any | null = null;\n  private modelLoaded: boolean = false;\n\n  constructor(db: Pool) {\n    this.db = db;\n    this.initializeModels();\n  }\n\n  /**\n   * Initialize TensorFlow models\n   */\n  private async initializeModels() {\n    try {\n      // In production, would load custom trained models\n      // For now, using pre-trained COCO-SSD for object detection\n      logger.info('Initializing AI detection models...');\n\n      // Would load models like:\n      // - Driver face/pose detection model\n      // - Phone/device detection model\n      // - Seatbelt detection model\n      // - Drowsiness detection model\n\n      this.modelLoaded = true;\n      logger.info('AI detection models initialized');\n    } catch (error: any) {\n      logger.error('Failed to initialize AI models:', error.message);\n    }\n  }\n\n  /**\n   * Analyze image for driver safety violations\n   */\n  async analyzeImage(imageData: Buffer | string): Promise<DetectionResult> {\n    const startTime = Date.now();\n    const detections: Detection[] = [];\n\n    try {\n      // Convert image to format suitable for analysis\n      const imageBase64 = typeof imageData === 'string'\n        ? imageData\n        : `data:image/jpeg;base64,${imageData.toString('base64')}`;\n\n      // Run multiple detection models in parallel\n      const [\n        objectDetections,\n        faceAnalysis,\n        poseAnalysis\n      ] = await Promise.all([\n        this.detectObjects(imageBase64),\n        this.analyzeFaceAttributes(imageBase64),\n        this.analyzePose(imageBase64)\n      ]);\n\n      // Process object detections\n      detections.push(...objectDetections);\n\n      // Process face analysis\n      if (faceAnalysis) {\n        detections.push(...faceAnalysis);\n      }\n\n      // Process pose analysis\n      if (poseAnalysis) {\n        detections.push(...poseAnalysis);\n      }\n\n      // Calculate overall risk score\n      const riskScore = this.calculateRiskScore(detections);\n\n      // Calculate average confidence\n      const confidence = detections.length > 0\n        ? detections.reduce((sum, d) => sum + d.confidence, 0) / detections.length\n        : 0;\n\n      const processingTime = Date.now() - startTime;\n\n      return {\n        detections,\n        riskScore,\n        confidence,\n        processingTime\n      };\n    } catch (error: any) {\n      logger.error('Image analysis failed:', error.message);\n      throw error;\n    }\n  }\n\n  /**\n   * Detect objects in image (phone, cigarette, food, etc.)\n   */\n  private async detectObjects(imageBase64: string): Promise<Detection[]> {\n    const detections: Detection[] = [];\n\n    try {\n      // Use Azure Computer Vision for object detection\n      if (!AZURE_COMPUTER_VISION_KEY || !AZURE_COMPUTER_VISION_ENDPOINT) {\n        logger.warn('Azure Computer Vision not configured');\n        return detections;\n      }\n\n      const response = await axios.post(\n        `${AZURE_COMPUTER_VISION_ENDPOINT}/vision/v3.2/detect`,\n        { url: imageBase64 },\n        {\n          headers: {\n            'Ocp-Apim-Subscription-Key': AZURE_COMPUTER_VISION_KEY,\n            'Content-Type': 'application/json'\n          }\n        }\n      );\n\n      if (response.data && response.data.objects) {\n        for (const obj of response.data.objects) {\n          const objName = obj.object.toLowerCase();\n          const confidence = obj.confidence || 0;\n\n          // Phone detection\n          if ((objName.includes('phone') || objName.includes('cell') || objName.includes('mobile')) && confidence > 0.7) {\n            detections.push({\n              type: 'phone_use',\n              category: 'distraction',\n              severity: 'severe',\n              confidence,\n              boundingBox: obj.rectangle,\n              metadata: { object: obj.object }\n            });\n          }\n\n          // Cigarette/smoking detection\n          if ((objName.includes('cigarette') || objName.includes('smoking')) && confidence > 0.65) {\n            detections.push({\n              type: 'smoking',\n              category: 'violation',\n              severity: 'moderate',\n              confidence,\n              boundingBox: obj.rectangle,\n              metadata: { object: obj.object }\n            });\n          }\n\n          // Food/drink detection\n          if ((objName.includes('food') || objName.includes('drink') || objName.includes('cup') || objName.includes('bottle')) && confidence > 0.65) {\n            detections.push({\n              type: 'eating_drinking',\n              category: 'distraction',\n              severity: 'minor',\n              confidence,\n              boundingBox: obj.rectangle,\n              metadata: { object: obj.object }\n            });\n          }\n\n          // Book/tablet detection\n          if ((objName.includes('book') || objName.includes('tablet') || objName.includes('magazine')) && confidence > 0.7) {\n            detections.push({\n              type: 'reading',\n              category: 'distraction',\n              severity: 'severe',\n              confidence,\n              boundingBox: obj.rectangle,\n              metadata: { object: obj.object }\n            });\n          }\n        }\n      }\n    } catch (error: any) {\n      logger.error('Object detection failed:', error.message);\n    }\n\n    return detections;\n  }\n\n  /**\n   * Analyze face attributes for drowsiness and attention\n   */\n  private async analyzeFaceAttributes(imageBase64: string): Promise<Detection[]> {\n    const detections: Detection[] = [];\n\n    try {\n      // Use OpenAI Vision API for advanced face analysis\n      if (!OPENAI_API_KEY) {\n        logger.warn('OpenAI API key not configured');\n        return detections;\n      }\n\n      const response = await axios.post(\n        'https://api.openai.com/v1/chat/completions',\n        {\n          model: 'gpt-4o',\n          messages: [\n            {\n              role: 'system',\n              content: 'You are a driver safety AI. Analyze the image and detect: eyes closed, yawning, looking away from road, drowsiness signs, distraction. Return JSON only with keys: eyesClosed (boolean), yawning (boolean), lookingAway (boolean), drowsinessScore (0-1), distractionScore (0-1), wearing_seatbelt (boolean).'\n            },\n            {\n              role: 'user',\n              content: [\n                {\n                  type: 'image_url',\n                  image_url: { url: imageBase64 }\n                },\n                {\n                  type: 'text',\n                  text: 'Analyze this driver image for safety issues.'\n                }\n              ]\n            }\n          ],\n          max_tokens: 300,\n          temperature: 0.1\n        },\n        {\n          headers: {\n            'Authorization': `Bearer ${OPENAI_API_KEY}`,\n            'Content-Type': 'application/json'\n          }\n        }\n      );\n\n      if (response.data.choices && response.data.choices[0].message.content) {\n        const content = response.data.choices[0].message.content;\n        const analysis = JSON.parse(content);\n\n        // Drowsiness detection\n        if (analysis.eyesClosed && analysis.drowsinessScore > 0.6) {\n          detections.push({\n            type: 'eyes_closed',\n            category: 'drowsiness',\n            severity: 'critical',\n            confidence: analysis.drowsinessScore,\n            metadata: { drowsinessScore: analysis.drowsinessScore }\n          });\n        }\n\n        // Yawning detection\n        if (analysis.yawning) {\n          detections.push({\n            type: 'yawning',\n            category: 'drowsiness',\n            severity: 'moderate',\n            confidence: 0.8,\n            metadata: {}\n          });\n        }\n\n        // Distraction detection\n        if (analysis.lookingAway && analysis.distractionScore > 0.7) {\n          detections.push({\n            type: 'looking_away',\n            category: 'distraction',\n            severity: 'severe',\n            confidence: analysis.distractionScore,\n            metadata: { distractionScore: analysis.distractionScore }\n          });\n        }\n\n        // Seatbelt detection\n        if (analysis.wearing_seatbelt === false) {\n          detections.push({\n            type: 'no_seatbelt',\n            category: 'violation',\n            severity: 'critical',\n            confidence: 0.85,\n            metadata: {}\n          });\n        }\n      }\n    } catch (error: any) {\n      logger.error('Face analysis failed:', error.message);\n    }\n\n    return detections;\n  }\n\n  /**\n   * Analyze body pose for driver positioning\n   */\n  private async analyzePose(imageBase64: string): Promise<Detection[]> {\n    const detections: Detection[] = [];\n\n    try {\n      // In production, would use PoseNet or similar model\n      // For now, using OpenAI Vision for pose analysis\n\n      if (!OPENAI_API_KEY) {\n        return detections;\n      }\n\n      const response = await axios.post(\n        'https://api.openai.com/v1/chat/completions',\n        {\n          model: 'gpt-4o',\n          messages: [\n            {\n              role: 'system',\n              content: 'Analyze driver body position. Detect: hand position (both hands on wheel, one hand, no hands), body position (leaning, slouching, proper position). Return JSON with: handsOnWheel (number 0-2), properPosition (boolean), leaning (boolean), handsNearFace (boolean).'\n            },\n            {\n              role: 'user',\n              content: [\n                {\n                  type: 'image_url',\n                  image_url: { url: imageBase64 }\n                },\n                {\n                  type: 'text',\n                  text: 'Analyze driver position.'\n                }\n              ]\n            }\n          ],\n          max_tokens: 200,\n          temperature: 0.1\n        },\n        {\n          headers: {\n            'Authorization': `Bearer ${OPENAI_API_KEY}`,\n            'Content-Type': 'application/json'\n          }\n        }\n      );\n\n      if (response.data.choices && response.data.choices[0].message.content) {\n        const content = response.data.choices[0].message.content;\n        const analysis = JSON.parse(content);\n\n        // Hand position check\n        if (analysis.handsOnWheel === 0) {\n          detections.push({\n            type: 'no_hands_on_wheel',\n            category: 'unsafe_driving',\n            severity: 'critical',\n            confidence: 0.9,\n            metadata: {}\n          });\n        } else if (analysis.handsOnWheel === 1) {\n          detections.push({\n            type: 'one_hand_driving',\n            category: 'unsafe_driving',\n            severity: 'minor',\n            confidence: 0.75,\n            metadata: {}\n          });\n        }\n\n        // Hands near face (potential phone use or distraction)\n        if (analysis.handsNearFace) {\n          detections.push({\n            type: 'hands_near_face',\n            category: 'distraction',\n            severity: 'moderate',\n            confidence: 0.7,\n            metadata: {}\n          });\n        }\n\n        // Poor driving posture\n        if (!analysis.properPosition || analysis.leaning) {\n          detections.push({\n            type: 'poor_posture',\n            category: 'unsafe_driving',\n            severity: 'minor',\n            confidence: 0.65,\n            metadata: { leaning: analysis.leaning }\n          });\n        }\n      }\n    } catch (error: any) {\n      logger.error('Pose analysis failed:', error.message);\n    }\n\n    return detections;\n  }\n\n  /**\n   * Calculate overall risk score from detections\n   */\n  private calculateRiskScore(detections: Detection[]): number {\n    if (detections.length === 0) {\n      return 0;\n    }\n\n    const severityWeights = {\n      minor: 1,\n      moderate: 2,\n      severe: 4,\n      critical: 8\n    };\n\n    const totalScore = detections.reduce((sum, detection) => {\n      const weight = severityWeights[detection.severity];\n      return sum + (detection.confidence * weight);\n    }, 0);\n\n    const maxPossibleScore = detections.length * 8; // Max weight is 8\n    return Math.min((totalScore / maxPossibleScore) * 100, 100);\n  }\n\n  /**\n   * Batch analyze multiple images\n   */\n  async batchAnalyze(images: Array<Buffer | string>): Promise<DetectionResult[]> {\n    const results: DetectionResult[] = [];\n\n    for (const image of images) {\n      try {\n        const result = await this.analyzeImage(image);\n        results.push(result);\n      } catch (error: any) {\n        logger.error('Batch analysis failed for image:', error.message);\n      }\n    }\n\n    return results;\n  }\n\n  /**\n   * Update detection model performance metrics\n   */\n  async updateModelMetrics(modelType: string, metrics: {\n    accuracy?: number;\n    falsePositiveRate?: number;\n    avgProcessingTime?: number;\n  }): Promise<void> {\n    try {\n      await this.db.query(\n        `INSERT INTO ai_detection_models (model_name, model_type, accuracy_rate, false_positive_rate, avg_processing_time_ms, total_detections)\n         VALUES ($1, $2, $3, $4, $5, 1)\n         ON CONFLICT (model_name)\n         DO UPDATE SET\n           accuracy_rate = COALESCE($3, ai_detection_models.accuracy_rate),\n           false_positive_rate = COALESCE($4, ai_detection_models.false_positive_rate),\n           avg_processing_time_ms = COALESCE($5, ai_detection_models.avg_processing_time_ms),\n           total_detections = ai_detection_models.total_detections + 1,\n           updated_at = NOW()`,\n        [\n          modelType,\n          'safety_detection',\n          metrics.accuracy,\n          metrics.falsePositiveRate,\n          metrics.avgProcessingTime\n        ]\n      );\n    ",
  "timestamp": "2026-01-18T16:21:48.534166"
}