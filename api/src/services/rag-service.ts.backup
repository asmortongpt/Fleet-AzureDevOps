/**
 * RAG (Retrieval Augmented Generation) Service
 *
 * Provides context-aware AI responses by:
 * 1. Storing conversation history with embeddings in Azure PostgreSQL
 * 2. Retrieving relevant context from past conversations
 * 3. Injecting context into LLM prompts for better responses
 *
 * All data stored in Azure (PostgreSQL + vector embeddings)
 * NO local file dependencies
 */

import { OpenAIEmbeddings, ChatOpenAI } from '@langchain/openai'
import { MemoryVectorStore } from '@langchain/community/vectorstores/memory'
import { Document } from '@langchain/core/documents'
import pool from '../config/database'

// Azure-backed vector store (in-memory with PostgreSQL persistence)
let vectorStore: MemoryVectorStore | null = null

interface ConversationContext {
  conversationId: string
  tenantId: string
  userId: string
  messages: Array<{
    role: 'user' | 'assistant' | 'system'
    content: string
    timestamp: Date
  }>
  metadata?: Record<string, any>
}

interface RAGConfig {
  maxContextMessages: number
  similarityThreshold: number
  embeddingModel: string
}

const defaultConfig: RAGConfig = {
  maxContextMessages: 10,
  similarityThreshold: 0.7,
  embeddingModel: 'text-embedding-ada-002'
}

/**
 * Initialize RAG service with OpenAI embeddings
 * Loads existing conversations from Azure PostgreSQL
 */
export async function initializeRAG(): Promise<void> {
  try {
    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: defaultConfig.embeddingModel
    })

    // Load existing conversations from Azure PostgreSQL database
    const existingDocs = await loadConversationsFromDatabase()

    vectorStore = await MemoryVectorStore.fromDocuments(
      existingDocs,
      embeddings
    )

    console.log('[RAG] Initialized with', existingDocs.length, 'conversation documents from Azure PostgreSQL')
  } catch (error) {
    console.error('[RAG] Initialization error:', error)
    // Fall back to empty vector store
    const embeddings = new OpenAIEmbeddings({
      openAIApiKey: process.env.OPENAI_API_KEY || 'dummy-key',
      modelName: defaultConfig.embeddingModel
    })
    vectorStore = await MemoryVectorStore.fromDocuments([], embeddings)
  }
}

/**
 * Load conversation history from Azure PostgreSQL (cloud storage)
 * NO local file dependencies - all data in Azure
 */
async function loadConversationsFromDatabase(): Promise<Document[]> {
  try {
    const result = await pool.query(`
      SELECT
        conversation_id,
        tenant_id,
        user_id,
        messages,
        extracted_data,
        created_at,
        updated_at
      FROM ai_conversations
      WHERE status = 'completed'
      AND updated_at > NOW() - INTERVAL '30 days'
      ORDER BY updated_at DESC
      LIMIT 1000
    `)

    return result.rows.map(row => {
      const messageText = Array.isArray(row.messages)
        ? row.messages.map((m: any) => `${m.role}: ${m.content}`).join('\n')
        : ''

      return new Document({
        pageContent: messageText,
        metadata: {
          conversationId: row.conversation_id,
          tenantId: row.tenant_id,
          userId: row.user_id,
          extractedData: row.extracted_data,
          createdAt: row.created_at,
          updatedAt: row.updated_at
        }
      })
    })
  } catch (error) {
    console.error('[RAG] Error loading conversations from Azure database:', error)
    return []
  }
}

/**
 * Add conversation to RAG context (with embedding)
 * Stores in Azure PostgreSQL for persistence
 */
export async function addConversationToRAG(
  conversationId: string,
  tenantId: string,
  userId: string,
  messages: Array<{ role: string; content: string }>,
  metadata?: Record<string, any>
): Promise<void> {
  if (!vectorStore) {
    await initializeRAG()
  }

  try {
    const messageText = messages.map(m => `${m.role}: ${m.content}`).join('\n')

    const doc = new Document({
      pageContent: messageText,
      metadata: {
        conversationId,
        tenantId,
        userId,
        timestamp: new Date().toISOString(),
        ...metadata
      }
    })

    await vectorStore!.addDocuments([doc])

    console.log('[RAG] Added conversation to vector store (Azure-backed):', conversationId)
  } catch (error) {
    console.error('[RAG] Error adding conversation:', error)
  }
}

/**
 * Retrieve relevant context for a query using RAG
 * Searches across all tenant conversations stored in Azure
 */
export async function getRelevantContext(
  query: string,
  tenantId: string,
  maxResults: number = 5
): Promise<string> {
  if (!vectorStore) {
    await initializeRAG()
  }

  try {
    // Perform similarity search across Azure-stored conversations
    const results = await vectorStore!.similaritySearch(query, maxResults)

    // Filter by tenant for multi-tenancy security
    const tenantResults = results.filter(
      (doc: Document) => doc.metadata.tenantId === tenantId
    )

    if (tenantResults.length === 0) {
      return 'No relevant conversation history found.'
    }

    // Format context for LLM injection
    const context = tenantResults
      .map((doc: Document, i: number) => {
        return `[Context ${i + 1} - ${new Date(doc.metadata.timestamp).toLocaleDateString()}]\n${doc.pageContent}`
      })
      .join('\n\n---\n\n')

    return context
  } catch (error) {
    console.error('[RAG] Error retrieving context:', error)
    return 'Error retrieving conversation history.'
  }
}

/**
 * Get conversation context from Azure PostgreSQL
 */
export async function getConversationContext(
  conversationId: string,
  tenantId: string
): Promise<ConversationContext | null> {
  try {
    const result = await pool.query(
      `SELECT
         conversation_id,
         tenant_id,
         user_id,
         messages,
         extracted_data,
         created_at,
         updated_at
       FROM ai_conversations
       WHERE conversation_id = $1 AND tenant_id = $2
       ORDER BY updated_at DESC
       LIMIT 1`,
      [conversationId, tenantId]
    )

    if (result.rows.length === 0) {
      return null
    }

    const row = result.rows[0]
    return {
      conversationId: row.conversation_id,
      tenantId: row.tenant_id,
      userId: row.user_id,
      messages: row.messages || [],
      metadata: row.extracted_data
    }
  } catch (error) {
    console.error('[RAG] Error getting conversation context from Azure:', error)
    return null
  }
}

/**
 * Generate AI response with RAG-enhanced context
 * Uses Azure-stored conversation history for better responses
 */
export async function generateRAGResponse(
  userMessage: string,
  tenantId: string,
  conversationId?: string,
  systemPrompt?: string
): Promise<{
  response: string
  context: string
  sources: string[]
}> {
  try {
    // 1. Retrieve relevant context using RAG from Azure
    const relevantContext = await getRelevantContext(userMessage, tenantId)

    // 2. Get specific conversation history from Azure PostgreSQL
    let conversationHistory = ''
    if (conversationId) {
      const ctx = await getConversationContext(conversationId, tenantId)
      if (ctx) {
        conversationHistory = ctx.messages
          .slice(-defaultConfig.maxContextMessages)
          .map(m => `${m.role}: ${m.content}`)
          .join('\n')
      }
    }

    // 3. Build enhanced prompt with Azure-retrieved context
    const enhancedPrompt = `
${systemPrompt || 'You are a helpful AI assistant for a fleet management system.'}

RELEVANT CONTEXT FROM PAST CONVERSATIONS (Azure-stored):
${relevantContext}

${conversationHistory ? `CURRENT CONVERSATION HISTORY:\n${conversationHistory}\n` : ''}

USER MESSAGE:
${userMessage}

Provide a helpful response using the context above when relevant. If the context doesn't apply, respond based on general knowledge about fleet management.
`

    // 4. Generate response using OpenAI with RAG context
    const model = new ChatOpenAI({
      openAIApiKey: process.env.OPENAI_API_KEY,
      modelName: 'gpt-4-turbo-preview',
      temperature: 0.7
    })

    const result = await model.invoke(enhancedPrompt)
    const response = typeof result.content === 'string' ? result.content : JSON.stringify(result.content)

    return {
      response: response.trim(),
      context: relevantContext,
      sources: extractSources(relevantContext)
    }
  } catch (error: any) {
    console.error('[RAG] Error generating response:', error)
    throw new Error(`RAG response generation failed: ${error.message}`)
  }
}

/**
 * Extract source references from context
 */
function extractSources(context: string): string[] {
  const matches = context.match(/\[Context \d+ - ([^\]]+)\]/g)
  return matches ? matches.map(m => m.replace(/\[Context \d+ - /, '').replace(']', '')) : []
}

/**
 * Persist conversation embeddings to Azure PostgreSQL
 * Ensures RAG context survives pod restarts (cloud-native)
 */
export async function persistConversationEmbedding(
  conversationId: string,
  tenantId: string,
  embedding: number[]
): Promise<void> {
  try {
    await pool.query(
      `UPDATE ai_conversations
       SET embedding = $1
       WHERE conversation_id = $2 AND tenant_id = $3`,
      [JSON.stringify(embedding), conversationId, tenantId]
    )
    console.log('[RAG] Persisted embedding to Azure PostgreSQL:', conversationId)
  } catch (error) {
    console.error('[RAG] Error persisting embedding to Azure:', error)
  }
}

// Initialize RAG on module load - loads data from Azure
initializeRAG().catch(console.error)

export default {
  initializeRAG,
  addConversationToRAG,
  getRelevantContext,
  getConversationContext,
  generateRAGResponse,
  persistConversationEmbedding
}
