[
  {
    "source": "frontend",
    "sheet": "Performance_n_Optimization",
    "row": 2,
    "key_metric": "- Bundle Size",
    "scope": "Application Level",
    "module_component": "- images\n- css\n- individual componets loading",
    "finding": "-  (All 50+ components in single bundle)\n- All routes loaded on initial page load\n- assets not optimized to cater responsiveness\n- user downloads the whole bundle even if they are working on 1-2 pages max",
    "severity": "High",
    "impact": "- with growing images and oter assets the size may still grow up ",
    "solution": "- need to implement lazu loading , responsive sizes, \n- implement code splitting\n- list vitualization for large lists (like the one in assetmanagement)\n- add suspense wrapper with global loading component ",
    "notes": "nan",
    "hours": "32",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "frontend",
    "sheet": "Performance_n_Optimization",
    "row": 3,
    "key_metric": "React Compiler",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- we have like 120 + instances where we are using usememo, usecallback",
    "severity": "High",
    "impact": "-ever growing boiler plate code that can be eliminated with just one library usage",
    "solution": "Use react compiler to Automatically memoizes components and values: \n- Enforces Rules of Hooks at compile time\n- Eliminates need for manual `useMemo`, `useCallback`, `React.memo`\n- Reduces boilerplate by 30-40%\n- Prevents common performance pitfalls",
    "notes": "nan",
    "hours": "24",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "frontend",
    "sheet": "Performance_n_Optimization",
    "row": 4,
    "key_metric": "Utility functions",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- no utlity functions impelemented even when we have amost 70+ data formatting calls, 25+ number formatiing calls, validation login in 15+ compoenents , export logic in 8+ compoenents ",
    "severity": "High",
    "impact": "- duplication code with so many touch points to make changes if needed ",
    "solution": "- need to have centralized date , numebr , export utilities",
    "notes": "nan",
    "hours": "40",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "frontend",
    "sheet": "Performance_n_Optimization",
    "row": 5,
    "key_metric": "Custom Hooks ",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- need to data management, state and business level hooks ",
    "severity": "High",
    "impact": "- duplication code with so many touch points to make changes if needed ",
    "solution": "- Data Management Hooks (useVehicleFilters, useSorting, usePagination etc)\n- UI State Hooks (useFormState, useTableState, useDialogState etc)",
    "notes": "nan",
    "hours": "already covered",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "backend",
    "sheet": "Performance_n_Optimization",
    "row": 3,
    "key_metric": "N+1 Query Patterns",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- Multiple routes fetch related data in loops",
    "severity": "High",
    "impact": "-**Impact**: For 50 communications, this becomes 150 queries!\n\n- **File**: `api/src/routes/communications.ts:127-147`\n   ```typescript\n   // \u26a0\ufe0f BAD: 3 separate queries per communication\n   const result = await pool.query(\n     'SELECT * FROM communications WHERE id = $1 AND tenant_id = $2',\n     [req.params.id, req.user!.tenant_id]\n   )\n   \n   // Query 2: Get linked entities\n   const linksResult = await pool.query(\n     `SELECT * FROM communication_entity_links WHERE communication_id = $1`,\n     [req.params.id]\n   )\n   \n   // Query 3: Get attachments\n   const attachmentsResult = await pool.query(\n     `SELECT * FROM communication_attachments WHERE communication_id = $1`,\n     [req.params.id]\n   )",
    "solution": "**Remediation**: Use JOINs or batch queries\n   ```typescript\n   // \u2705 GOOD: Single query with JOINs\n   const result = await pool.query(`\n     SELECT \n       c.*,\n       json_agg(DISTINCT jsonb_build_object(\n         'entity_type', cel.entity_type,\n         'entity_id', cel.entity_id,\n         'link_type', cel.link_type\n       )) FILTER (WHERE cel.id IS NOT NULL) as linked_entities,\n       json_agg(DISTINCT jsonb_build_object(\n         'id', ca.id,\n         'filename', ca.filename,\n         'url', ca.url\n       )) FILTER (WHERE ca.id IS NOT NULL) as attachments\n     FROM communications c\n     LEFT JOIN communication_entity_links cel ON c.id = cel.communication_id\n     LEFT JOIN communication_attachments ca ON c.id = ca.communication_id\n     WHERE c.id = $1 AND c.tenant_id = $2\n     GROUP BY c.id\n   `, [req.params.id, req.user!.tenant_id])",
    "notes": "nan",
    "hours": "40.0",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "backend",
    "sheet": "Performance_n_Optimization",
    "row": 4,
    "key_metric": "Have API response middleware",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- no measure on which api's are slow and need to be looked into",
    "severity": "High",
    "impact": "-Can't identify slow endpoints",
    "solution": "- we can add response time middleware in the start before we have a hold on every api performance matching our benchmark",
    "notes": "nan",
    "hours": "16.0",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  },
  {
    "source": "backend",
    "sheet": "Performance_n_Optimization",
    "row": 6,
    "key_metric": "Worker Threads for CPU intensive tasks ",
    "scope": "Application Level",
    "module_component": "nan",
    "finding": "- not using worker threads at all",
    "severity": "High",
    "impact": "-  All requests blocked during generation/processing\n-  event loop gets choked",
    "solution": "- we can identify places to use worker thread ,s pecially where there are : \n- CPU-intensive computations (image processing, encryption) - external AI and other tool calls (if needed)\n- Heavy JSON parsing/serialization\n- Data transformation (large datasets)\n- Cryptographic operations",
    "notes": "nan",
    "hours": "32.0",
    "completion_pct": "nan",
    "status": "nan",
    "next_steps": "nan"
  }
]